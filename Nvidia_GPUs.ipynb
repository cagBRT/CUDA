{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100",
      "authorship_tag": "ABX9TyNEJqmRsCvC0rc0kYbQJemJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cagBRT/CUDA/blob/main/Nvidia_GPUs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Select the GPU to use by going to<br>\n",
        "\n",
        "Runtime>Change Runtime Type><br>\n",
        "Then select a GPU"
      ],
      "metadata": {
        "id": "J3sXTKzmnGrY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This output provides information about the NVIDIA GPU that’s currently allocated to your Colab session."
      ],
      "metadata": {
        "id": "10X_6ZpvndJh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JnRbCSLYm0R4",
        "outputId": "8f749248-90f8-4fe4-9c04-d3964795847e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri Apr 12 19:33:23 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla V100-SXM2-16GB           Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   38C    P0              27W / 300W |      0MiB / 16384MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Google Colab’s Pros and Cons<br>\n",
        "**Pros**<br>\n",
        "Free Access to GPUs: Google Colab provides free access to GPUs, making it an attractive platform for data scientists and software engineers who want to leverage powerful computing resources without incurring additional costs.<br>\n",
        "\n",
        "Cloud-Based Platform: Being a cloud-based platform, Google Colab eliminates the need for users to set up and maintain their own computing infrastructure. This allows for easier collaboration and seamless sharing of Jupyter notebooks.<br>\n",
        "\n",
        "Ease of Use: Colab is known for its user-friendly interface and seamless integration with Jupyter notebooks. Users can quickly set up and run machine learning experiments without the hassle of configuring hardware or software.<br>\n",
        "\n",
        "Wide Adoption: Due to its free GPU access and ease of use, Google Colab has gained widespread adoption among the machine learning community. This popularity fosters a supportive user community and ensures that users can find resources and help easily.\n",
        "\n",
        "**Cons**<br>\n",
        "Variable GPU Specs: The allocated GPU specifications in Google Colab can vary, and users may not always have clarity on the available resources. This variability can impact the performance and efficiency of machine learning experiments.<br>\n",
        "\n",
        "Limited GPU Memory: The allocated GPU memory may not always be sufficient for complex machine learning models or large datasets. Users may face limitations that hinder their ability to train certain types of models or handle extensive datasets.<br>\n",
        "\n",
        "Resource Availability: Larger GPUs may not always be available, and requesting a change in GPU type or size might result in a longer wait time before the Colab session is ready. This can be a drawback, especially when time is crucial for the experimentation process.\n",
        "\n"
      ],
      "metadata": {
        "id": "N7iP-fIlnpWD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Error Handling\n",
        "**Unavailable GPUs**: Users should be prepared to handle situations where GPUs are not available due to high demand or maintenance. It’s advisable to have contingency plans, such as trying the experiment at a later time or exploring alternative platforms.\n",
        "\n",
        "**Error in GPU Allocation**: If there is an error in GPU allocation, users should check their Colab runtime settings and ensure that they have selected the correct GPU type. Clear instructions on troubleshooting such errors can help users quickly resolve issues.\n",
        "\n",
        "**Memory Exhaustion**: In cases where GPU memory is insufficient, users should consider optimizing their machine learning models, utilizing data batching techniques, or exploring alternatives like distributed computing. Clear guidance on managing memory issues can enhance the user experience.\n",
        "\n",
        "**Long Wait Times**: Users requesting larger GPUs should be aware of potential longer wait times. Adequate communication about expected wait times can help manage user expectations and allow them to plan their work accordingly.\n",
        "\n"
      ],
      "metadata": {
        "id": "Bg_nq2phn8us"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Google Colab has a new paid tier, [Pay As You Go](https://colab.research.google.com/signup), giving anyone the option to purchase additional compute time in Colab with or without a paid subscription. This grants access to Colab’s  powerful NVIDIA GPUs and gives you more control over your machine learning environment."
      ],
      "metadata": {
        "id": "sjFvCyPiorpB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# To show that if there is cuda tookit installed\n",
        "!ls /usr/local"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vQDCGa8Os6q4",
        "outputId": "da8e6e53-536d-44c7-a5e6-4a06b8ebb8ce"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bin    cuda\tcuda-12.2  games\t       include\tlib64\t   man\t share\n",
            "colab  cuda-12\tetc\t   _gcs_config_ops.so  lib\tlicensing  sbin  src\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# To show that if we have the nvcc command\n",
        "!which nvcc"
      ],
      "metadata": {
        "id": "5anPMNjFs9vb"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To show the property of the nvidia card(On my one, I use the K80)\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V-z9zHsutByT",
        "outputId": "671272b2-1d4f-4cd9-92b3-5358b0413515"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri Apr 12 19:33:23 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla V100-SXM2-16GB           Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   38C    P0              27W / 300W |      0MiB / 16384MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Write a cu file contain the host and kernel code\n",
        "%%writefile hello.cu\n",
        "\n",
        "#include <cstdio>\n",
        "\n",
        "__global__ void hello(void)\n",
        "{\n",
        "  printf(\"GPU: Hello!\\n\");\n",
        "}\n",
        "\n",
        "int main(int argc,char **argv)\n",
        "{\n",
        "  printf(\"CPU: Hello!\\n\");\n",
        "  hello<<<1,10>>>();\n",
        "  cudaDeviceReset();\n",
        "  return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1JjuBFbwtGsn",
        "outputId": "71f28f90-6bb7-47cb-dbff-6b2abdaeb5b8"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting hello.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!nvcc --help"
      ],
      "metadata": {
        "id": "PHaq1PxktmH6"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc --list-gpu-arch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "quKZMiBituha",
        "outputId": "4c5d886e-366f-4d47-a7db-1862b5a290fc"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: nvcc: command not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the code. The flag is needed if you use the Tesla K80.\n",
        "!nvcc -arch=sm_50 -gencode=arch=compute_50,code=sm_50 hello.cu -o hello"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6BEEPZJltLEF",
        "outputId": "8751025c-4d4d-404a-f38b-13d101c78f34"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: nvcc: command not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./hello"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HDWOj6X4tSyn",
        "outputId": "eb376bdf-3aea-4122-bd08-3c306ce4eb70"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU: Hello!\n",
            "GPU: Hello!\n",
            "GPU: Hello!\n",
            "GPU: Hello!\n",
            "GPU: Hello!\n",
            "GPU: Hello!\n",
            "GPU: Hello!\n",
            "GPU: Hello!\n",
            "GPU: Hello!\n",
            "GPU: Hello!\n",
            "GPU: Hello!\n"
          ]
        }
      ]
    }
  ]
}