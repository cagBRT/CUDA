{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100",
      "authorship_tag": "ABX9TyPgW6acMeOAUm+gY0JMjqBf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cagBRT/CUDA/blob/main/Nvidia_GPUs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Select the GPU to use by going to<br>\n",
        "\n",
        "Runtime>Change Runtime Type><br>\n",
        "Then select a GPU"
      ],
      "metadata": {
        "id": "J3sXTKzmnGrY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This output provides information about the NVIDIA GPU that’s currently allocated to your Colab session."
      ],
      "metadata": {
        "id": "10X_6ZpvndJh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JnRbCSLYm0R4",
        "outputId": "f314da7e-90ce-4ef8-a72c-b629f1cca38f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri Apr 12 20:10:07 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla V100-SXM2-16GB           Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   34C    P0              25W / 300W |      0MiB / 16384MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Google Colab’s Pros and Cons<br>\n",
        "**Pros**<br>\n",
        "Free Access to GPUs: Google Colab provides free access to GPUs, making it an attractive platform for data scientists and software engineers who want to leverage powerful computing resources without incurring additional costs.<br>\n",
        "\n",
        "Cloud-Based Platform: Being a cloud-based platform, Google Colab eliminates the need for users to set up and maintain their own computing infrastructure. This allows for easier collaboration and seamless sharing of Jupyter notebooks.<br>\n",
        "\n",
        "Ease of Use: Colab is known for its user-friendly interface and seamless integration with Jupyter notebooks. Users can quickly set up and run machine learning experiments without the hassle of configuring hardware or software.<br>\n",
        "\n",
        "Wide Adoption: Due to its free GPU access and ease of use, Google Colab has gained widespread adoption among the machine learning community. This popularity fosters a supportive user community and ensures that users can find resources and help easily.\n",
        "\n",
        "**Cons**<br>\n",
        "Variable GPU Specs: The allocated GPU specifications in Google Colab can vary, and users may not always have clarity on the available resources. This variability can impact the performance and efficiency of machine learning experiments.<br>\n",
        "\n",
        "Limited GPU Memory: The allocated GPU memory may not always be sufficient for complex machine learning models or large datasets. Users may face limitations that hinder their ability to train certain types of models or handle extensive datasets.<br>\n",
        "\n",
        "Resource Availability: Larger GPUs may not always be available, and requesting a change in GPU type or size might result in a longer wait time before the Colab session is ready. This can be a drawback, especially when time is crucial for the experimentation process.\n",
        "\n"
      ],
      "metadata": {
        "id": "N7iP-fIlnpWD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Error Handling\n",
        "**Unavailable GPUs**: Users should be prepared to handle situations where GPUs are not available due to high demand or maintenance. It’s advisable to have contingency plans, such as trying the experiment at a later time or exploring alternative platforms.\n",
        "\n",
        "**Error in GPU Allocation**: If there is an error in GPU allocation, users should check their Colab runtime settings and ensure that they have selected the correct GPU type. Clear instructions on troubleshooting such errors can help users quickly resolve issues.\n",
        "\n",
        "**Memory Exhaustion**: In cases where GPU memory is insufficient, users should consider optimizing their machine learning models, utilizing data batching techniques, or exploring alternatives like distributed computing. Clear guidance on managing memory issues can enhance the user experience.\n",
        "\n",
        "**Long Wait Times**: Users requesting larger GPUs should be aware of potential longer wait times. Adequate communication about expected wait times can help manage user expectations and allow them to plan their work accordingly.\n",
        "\n"
      ],
      "metadata": {
        "id": "Bg_nq2phn8us"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Google Colab has a new paid tier, [Pay As You Go](https://colab.research.google.com/signup), giving anyone the option to purchase additional compute time in Colab with or without a paid subscription. This grants access to Colab’s  powerful NVIDIA GPUs and gives you more control over your machine learning environment."
      ],
      "metadata": {
        "id": "sjFvCyPiorpB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# To show that if there is cuda tookit installed\n",
        "!ls /usr/local"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vQDCGa8Os6q4",
        "outputId": "faba4e54-cfd2-4313-ec3b-2288e7f23f60"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bin    cuda\tcuda-12.2  games\t       include\tlib64\t   man\t share\n",
            "colab  cuda-12\tetc\t   _gcs_config_ops.so  lib\tlicensing  sbin  src\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# To show that if we have the nvcc command\n",
        "!which nvcc"
      ],
      "metadata": {
        "id": "5anPMNjFs9vb",
        "outputId": "7454805b-ba96-4f66-df23-8e71a7bfa6e6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/cuda/bin/nvcc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# To show the property of the nvidia card(On my one, I use the K80)\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V-z9zHsutByT",
        "outputId": "c2e4185c-6bcf-4c8e-872b-36d3b2833ee7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri Apr 12 20:10:07 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla V100-SXM2-16GB           Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   34C    P0              25W / 300W |      0MiB / 16384MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Write a cu file contain the host and kernel code\n",
        "%%writefile hello.cu\n",
        "\n",
        "#include <cstdio>\n",
        "\n",
        "__global__ void hello(void)\n",
        "{\n",
        "  printf(\"GPU: Hello!\\n\");\n",
        "}\n",
        "\n",
        "int main(int argc,char **argv)\n",
        "{\n",
        "  printf(\"CPU: Hello!\\n\");\n",
        "  hello<<<1,10>>>();\n",
        "  cudaDeviceReset();\n",
        "  return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1JjuBFbwtGsn",
        "outputId": "128d5b40-dbc1-447c-ce48-1ab18c6acd47"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing hello.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!nvcc --help"
      ],
      "metadata": {
        "id": "PHaq1PxktmH6"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc --list-gpu-arch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "quKZMiBituha",
        "outputId": "3356428e-bbc4-4006-9f9d-dc2d436376b7"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "compute_50\n",
            "compute_52\n",
            "compute_53\n",
            "compute_60\n",
            "compute_61\n",
            "compute_62\n",
            "compute_70\n",
            "compute_72\n",
            "compute_75\n",
            "compute_80\n",
            "compute_86\n",
            "compute_87\n",
            "compute_89\n",
            "compute_90\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the code. The flag is needed if you use the Tesla K80.\n",
        "!nvcc -arch=sm_50 -gencode=arch=compute_50,code=sm_50 hello.cu -o hello"
      ],
      "metadata": {
        "id": "6BEEPZJltLEF"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./hello"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HDWOj6X4tSyn",
        "outputId": "7cd2f18d-ab4d-4e24-f06e-2abc444c4938"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU: Hello!\n",
            "GPU: Hello!\n",
            "GPU: Hello!\n",
            "GPU: Hello!\n",
            "GPU: Hello!\n",
            "GPU: Hello!\n",
            "GPU: Hello!\n",
            "GPU: Hello!\n",
            "GPU: Hello!\n",
            "GPU: Hello!\n",
            "GPU: Hello!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile example1.cu\n",
        "\n",
        "#include <cstdio>\n",
        "#include <iostream>\n",
        "\n",
        "using namespace std;\n",
        "\n",
        "__global__ void maxi(int* a, int* b, int n)\n",
        "{\n",
        "\tint block = 256 * blockIdx.x;\n",
        "\tint max = 0;\n",
        "\n",
        "\tfor (int i = block; i < min(256 + block, n); i++) {\n",
        "\n",
        "\t\tif (max < a[i]) {\n",
        "\t\t\tmax = a[i];\n",
        "\t\t}\n",
        "\t}\n",
        "\tb[blockIdx.x] = max;\n",
        "}\n",
        "\n",
        "int main()\n",
        "{\n",
        "\n",
        "\tint n;\n",
        "\tn = 3 >> 2;\n",
        "\tint a[n];\n",
        "\n",
        "\tfor (int i = 0; i < n; i++) {\n",
        "\t\ta[i] = rand() % n;\n",
        "\t\tcout << a[i] << \"\\t\";\n",
        "\t}\n",
        "\n",
        "\tcudaEvent_t start, end;\n",
        "\tint *ad, *bd;\n",
        "\tint size = n * sizeof(int);\n",
        "\tcudaMalloc(&ad, size);\n",
        "\tcudaMemcpy(ad, a, size, cudaMemcpyHostToDevice);\n",
        "\tint grids = ceil(n * 1.0f / 256.0f);\n",
        "\tcudaMalloc(&bd, grids * sizeof(int));\n",
        "\n",
        "\tdim3 grid(grids, 1);\n",
        "\tdim3 block(1, 1);\n",
        "\n",
        "\tcudaEventCreate(&start);\n",
        "\tcudaEventCreate(&end);\n",
        "\tcudaEventRecord(start);\n",
        "\n",
        "\twhile (n > 1) {\n",
        "\t\tmaxi<<<grids, block>>>(ad, bd, n);\n",
        "\t\tn = ceil(n * 1.0f / 256.0f);\n",
        "\t\tcudaMemcpy(ad, bd, n * sizeof(int), cudaMemcpyDeviceToDevice);\n",
        "\t}\n",
        "\n",
        "\tcudaEventRecord(end);\n",
        "\tcudaEventSynchronize(end);\n",
        "\n",
        "\tfloat time = 0;\n",
        "\tcudaEventElapsedTime(&time, start, end);\n",
        "\n",
        "\tint ans[2];\n",
        "\tcudaMemcpy(ans, ad, 4, cudaMemcpyDeviceToHost);\n",
        "\n",
        "\tcout << \"The maximum element is : \" << ans[0] << endl;\n",
        "\n",
        "\tcout << \"The time required : \";\n",
        "\tcout << time << endl;\n",
        "}\n"
      ],
      "metadata": {
        "id": "xTOfhmfwh3s4",
        "outputId": "ed279605-1cfb-4518-e643-53e62f2f0c2d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing example1.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -arch=sm_50 -gencode=arch=compute_50,code=sm_50 example1.cu -o example1"
      ],
      "metadata": {
        "id": "yKeQhBNo7_GZ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./example1"
      ],
      "metadata": {
        "id": "Kik7FfCJ76J8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile example2.cu\n",
        "\n",
        "#include <stdio.h>\n",
        "\n",
        "// This is a special function that runs on the GPU (device) instead of the CPU (host)\n",
        "__global__ void kernel() {\n",
        "  printf(\"Hello world!\\n\");\n",
        "}\n",
        "\n",
        "int main() {\n",
        "  // Invoke the kernel function on the GPU with one block of one thread\n",
        "  kernel<<<1,1>>>();\n",
        "\n",
        "  // Check for error codes (remember to do this for _every_ CUDA function)\n",
        "  if(cudaDeviceSynchronize() != cudaSuccess) {\n",
        "    fprintf(stderr, \"CUDA Error: %s\\n\", cudaGetErrorString(cudaPeekAtLastError()));\n",
        "  }\n",
        "  return 0;\n",
        "}"
      ],
      "metadata": {
        "id": "WoRsWBb-kHWn",
        "outputId": "981cf617-5fc7-47b0-fa37-6ea60a8d9f82",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing example2.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -arch=sm_50 -gencode=arch=compute_50,code=sm_50 example2.cu -o example2"
      ],
      "metadata": {
        "id": "KkcluDCH8TfK"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./example2"
      ],
      "metadata": {
        "id": "IZ09w0o08V4y",
        "outputId": "febdf5fa-f196-4c8d-8a51-714843e7cc4c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello world!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile example3.cu\n",
        "\n",
        "#include <stdio.h>\n",
        "\n",
        "// This kernel runs on the GPU and prints the thread's identifiers\n",
        "__global__ void kernel() {\n",
        "  printf(\"Hello from block %d thread %d\\n\", blockIdx.x, threadIdx.x);\n",
        "}\n",
        "\n",
        "int main() {\n",
        "  // Launch the kernel on the GPU with four blocks of six threads each\n",
        "  kernel<<<4,6>>>();\n",
        "\n",
        "  // Check for CUDA errors\n",
        "  if(cudaDeviceSynchronize() != cudaSuccess) {\n",
        "    fprintf(stderr, \"CUDA Error: %s\\n\", cudaGetErrorString(cudaPeekAtLastError()));\n",
        "  }\n",
        "  return 0;\n",
        "}"
      ],
      "metadata": {
        "id": "eKhIVZ2Ckdyb",
        "outputId": "565f6bf8-4c92-4e6c-e693-87bd36266038",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing example3.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -arch=sm_50 -gencode=arch=compute_50,code=sm_50 example3.cu -o example3"
      ],
      "metadata": {
        "id": "wFB_P3_E8kP-"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./example3"
      ],
      "metadata": {
        "id": "1oRjr5L28oMl",
        "outputId": "5aa3dd4e-af5a-48c8-bf80-ff982ab56a82",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello from block 3 thread 0\n",
            "Hello from block 3 thread 1\n",
            "Hello from block 3 thread 2\n",
            "Hello from block 3 thread 3\n",
            "Hello from block 3 thread 4\n",
            "Hello from block 3 thread 5\n",
            "Hello from block 0 thread 0\n",
            "Hello from block 0 thread 1\n",
            "Hello from block 0 thread 2\n",
            "Hello from block 0 thread 3\n",
            "Hello from block 0 thread 4\n",
            "Hello from block 0 thread 5\n",
            "Hello from block 2 thread 0\n",
            "Hello from block 2 thread 1\n",
            "Hello from block 2 thread 2\n",
            "Hello from block 2 thread 3\n",
            "Hello from block 2 thread 4\n",
            "Hello from block 2 thread 5\n",
            "Hello from block 1 thread 0\n",
            "Hello from block 1 thread 1\n",
            "Hello from block 1 thread 2\n",
            "Hello from block 1 thread 3\n",
            "Hello from block 1 thread 4\n",
            "Hello from block 1 thread 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile example4.cu\n",
        "\n",
        "#include <stdint.h>\n",
        "#include <stdio.h>\n",
        "\n",
        "#define N 32\n",
        "#define THREADS_PER_BLOCK 32\n",
        "\n",
        "__global__ void saxpy(float a, float* x, float* y) {\n",
        "  // Which index of the array should this thread use?\n",
        "  size_t index = 20;\n",
        "\n",
        "  // Compute a times x plus y for a specific index\n",
        "  y[index] = a * x[index] + y[index];\n",
        "}\n",
        "\n",
        "int main() {\n",
        "  // Allocate arrays for X and Y on the CPU. This memory is only usable on the CPU\n",
        "  float* cpu_x = (float*)malloc(sizeof(float) * N);\n",
        "  float* cpu_y = (float*)malloc(sizeof(float) * N);\n",
        "\n",
        "  // Initialize X and Y\n",
        "  int i;\n",
        "  for(i=0; i<N; i++) {\n",
        "    cpu_x[i] = (float)i;\n",
        "    cpu_y[i] = 0.0;\n",
        "  }\n",
        "\n",
        "  // The gpu_x and gpu_y pointers will only be usable on the GPU (which uses separate memory)\n",
        "  float* gpu_x;\n",
        "  float* gpu_y;\n",
        "\n",
        "  // Allocate space for the x array on the GPU\n",
        "  if(cudaMalloc(&gpu_x, sizeof(float) * N) != cudaSuccess) {\n",
        "    fprintf(stderr, \"Failed to allocate X array on GPU\\n\");\n",
        "    exit(2);\n",
        "  }\n",
        "\n",
        "  // Allocate space for the y array on the GPU\n",
        "  if(cudaMalloc(&gpu_y, sizeof(float) * N) != cudaSuccess) {\n",
        "    fprintf(stderr, \"Failed to allocate Y array on GPU\\n\");\n",
        "    exit(2);\n",
        "  }\n",
        "\n",
        "  // Copy the cpu's x array to the gpu with cudaMemcpy\n",
        "  if(cudaMemcpy(gpu_x, cpu_x, sizeof(float) * N, cudaMemcpyHostToDevice) != cudaSuccess) {\n",
        "    fprintf(stderr, \"Failed to copy X to the GPU\\n\");\n",
        "  }\n",
        "\n",
        "  // Copy the cpu's y array to the gpu with cudaMemcpy\n",
        "  if(cudaMemcpy(gpu_y, cpu_y, sizeof(float) * N, cudaMemcpyHostToDevice) != cudaSuccess) {\n",
        "    fprintf(stderr, \"Failed to copy Y to the GPU\\n\");\n",
        "  }\n",
        "\n",
        "  // Calculate the number of blocks to run, rounding up to include all threads\n",
        "  size_t blocks = (N + THREADS_PER_BLOCK - 1) / THREADS_PER_BLOCK;\n",
        "\n",
        "  // Run the saxpy kernel\n",
        "  saxpy<<<blocks, THREADS_PER_BLOCK>>>(0.5, gpu_x, gpu_y);\n",
        "\n",
        "  // Wait for the kernel to finish\n",
        "  if(cudaDeviceSynchronize() != cudaSuccess) {\n",
        "    fprintf(stderr, \"CUDA Error: %s\\n\", cudaGetErrorString(cudaPeekAtLastError()));\n",
        "  }\n",
        "\n",
        "  // Copy the y array back from the gpu to the cpu\n",
        "  if(cudaMemcpy(cpu_y, gpu_y, sizeof(float) * N, cudaMemcpyDeviceToHost) != cudaSuccess) {\n",
        "    fprintf(stderr, \"Failed to copy Y from the GPU\\n\");\n",
        "  }\n",
        "\n",
        "  // Print the updated y array\n",
        "  for(i=0; i<N; i++) {\n",
        "    printf(\"%d: %f\\n\", i, cpu_y[i]);\n",
        "  }\n",
        "\n",
        "  cudaFree(gpu_x);\n",
        "  cudaFree(gpu_y);\n",
        "  free(cpu_x);\n",
        "  free(cpu_y);\n",
        "\n",
        "  return 0;\n",
        "}\n"
      ],
      "metadata": {
        "id": "kKAZmeMekgWp",
        "outputId": "fa5d0761-4fb5-402e-be71-cdf0f6961530",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing example4.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -arch=sm_50 -gencode=arch=compute_50,code=sm_50 example4.cu -o example4"
      ],
      "metadata": {
        "id": "7OCdzGN68zJZ"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./example4"
      ],
      "metadata": {
        "id": "mx80FVXp84yk",
        "outputId": "5f5e1aa3-f106-44a8-85da-b38907a136a9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0: 0.000000\n",
            "1: 0.000000\n",
            "2: 0.000000\n",
            "3: 0.000000\n",
            "4: 0.000000\n",
            "5: 0.000000\n",
            "6: 0.000000\n",
            "7: 0.000000\n",
            "8: 0.000000\n",
            "9: 0.000000\n",
            "10: 0.000000\n",
            "11: 0.000000\n",
            "12: 0.000000\n",
            "13: 0.000000\n",
            "14: 0.000000\n",
            "15: 0.000000\n",
            "16: 0.000000\n",
            "17: 0.000000\n",
            "18: 0.000000\n",
            "19: 0.000000\n",
            "20: 10.000000\n",
            "21: 0.000000\n",
            "22: 0.000000\n",
            "23: 0.000000\n",
            "24: 0.000000\n",
            "25: 0.000000\n",
            "26: 0.000000\n",
            "27: 0.000000\n",
            "28: 0.000000\n",
            "29: 0.000000\n",
            "30: 0.000000\n",
            "31: 0.000000\n"
          ]
        }
      ]
    }
  ]
}